{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6OTwnf_lpr5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrNGH9E8lyw0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2TokenizerFast, AutoModelForCausalLM\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "aaf7b64295a441efb1e87444656dc59d",
            "8c923769beb64941b9b96a6534467945",
            "e686c16836c5461595c1481fb40630eb",
            "2e81b4d056f848d4bfcb8c0f142df17a",
            "fe9ac790911b4532976143c00f490e36",
            "8ab5bc6e22504d588b673cad8a2cc388",
            "2d14f9c903df43ee901d579878892428",
            "280015db528a4687bfc805747eb2a3b2",
            "c54cc4df4b9144f38f409cc3e719f0e8",
            "0980b45447174bbabd461b37b88835f5",
            "5bcdd0325393407da5840676c65fd6c9",
            "4c7fee6924ff4dcf98ea16648c8be86e",
            "f9688744e4ec4c51a18c21622b9fee85",
            "28a88eaac9904dbfb1f314c77ca250c3",
            "07e0d27fc35542df879243de520604ee",
            "537cec0d438c43608ef6a35b8045e937",
            "f25ae12ce84e4636b118ecfdfd117b5e",
            "bf9bdb3b893647729c5b5d14ed715d3b",
            "e93ab5f567ea4efdb08feedc149e4f2c",
            "27c03dd472ce4ab481b849db358bff7b",
            "0a9f2f55c56a4335bf3ae574e40bc47f",
            "310f634b6274482e9d00e1a86f3d2fbf"
          ]
        },
        "id": "HgRFJwAMl0ur",
        "outputId": "27041e27-0c6d-4db4-b282-73ff7c6c2ab1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf7b64295a441efb1e87444656dc59d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c7fee6924ff4dcf98ea16648c8be86e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        ")\n",
        "\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "print(\"Dataset loaded\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "088f88f0aff649ebb10c95ab285e4251",
            "9ea81e85f0174988b9b2c4f70d971277",
            "fccb6769870d40f59445105dc36abad6",
            "d7091a4f2b5c4522accdfdc03c44d8e3",
            "88ef93a1cc49438c86e2969f2491a0fb",
            "8c2b64d4c3ad4697a533257dc70e3b59",
            "4fed185918da4f7dbbf6988c247ef924",
            "c90890cb66c94aee8dae005cd16191ba",
            "fed54a7d7c6c42469d78ee89b5c43c87",
            "759ca26355144f37b709c1d9764cea0e",
            "40b4fedbb713488f888174e3bdabae9d",
            "538d52107f004aadaf49fa4e3e0d4107",
            "6c659a14f6bb40f2b834d863ca54c7bc",
            "0c5b2413cee0404a8d83d9b6d03e6c0d",
            "394d11ce6f7e4f189a23d3f62d5219b5",
            "b18543a0278a486b877578fcac783b32",
            "2b79ef731cfb4bf9aab93748647b45fa",
            "69e4533974b8450fb19aae639301c3b9",
            "ccde53459e674fe983954dab9ae4505a",
            "ab58dec6c67d43aeab9b294a9a4c4a0a",
            "2a7df0c4ad6a492c836d3ddcf049ed40",
            "87ed6bc4f4b24a598928e6c6ea3084b4",
            "d6a0ba1df6a846bc911360eddf315df0",
            "d6c1f7e0409e408980e7fb47c792c3d8",
            "5421f1e048054966b2a40a5a8934f158",
            "601521dc509b4c6dbae0ba258cbebc08",
            "6e5cf2f00a0a40a2a420ddc4bc00835d",
            "5bc11e4f33cc46f794c1c8d5eae802be",
            "bddf7a5bae724658bd4df86d7a08ffd2",
            "f992b00218694e279ddcd7bd18dbdd5e",
            "901ed963f34f4daf880dc946f027f041",
            "a5a34c59648545888abe7a3740e6156f",
            "8ebf525fd0e64dd284a11dc4bf2b007d",
            "dbe7d7a06f4a4960ba4728de0fc39aa4",
            "a1801d6d23514073b15cbd2013032bc4",
            "913e0545e965470ba58457d78689ce1c",
            "df283cfad63743d9ad733f923d19d817",
            "44b79dad4c3640afb2d7fbb5fb3d8ed7",
            "d3b9c1883768413ea62c05adb4a9cf02",
            "cd77b713db8d4261b424f6588d4d6430",
            "0cf0a94f18c145b9bef88a2368f315f8",
            "de72f96c2cb649c4b4c27adedd7582a0",
            "b9d1e75fba1d40239390faf88ca02617",
            "be5d777011244dcd8bfba7127f54ad52",
            "1157a9e0ab8140c9ac170e9b39e4b59c",
            "a14f736f406043cc97b2871ee708c8cd",
            "a2bf3226b4dc4f33a3ca4a4fc2cd15ca",
            "9cb363de444341aaad75fe7f5f9f9cba",
            "f33ef41de47b4f48926bb1315ac45d50",
            "7d79f24e5fe347e0b0d82e57359d212c",
            "dcb6db9753364416a9bebebb064bccdd",
            "ffe2c4caf9334412974160c1d7e00e88",
            "49caab878a0c4d2ba4cd00332e4974be",
            "8c28267c97c84290b48947eee6f5e4b1",
            "51e54866a5e0476ea21288b3c138224d"
          ]
        },
        "id": "GqR446-Fl4_U",
        "outputId": "850d86e4-3fe9-4b9d-f495-f6833494a082"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "088f88f0aff649ebb10c95ab285e4251",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538d52107f004aadaf49fa4e3e0d4107",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6a0ba1df6a846bc911360eddf315df0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbe7d7a06f4a4960ba4728de0fc39aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1157a9e0ab8140c9ac170e9b39e4b59c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer ready\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "bab4ed0d1ff747e5abc69996ae8bc1a8",
            "446d0587c7044ba5bd9ae11a0aa3d2b5",
            "669d466e010d4884b82c2d19ccd8fdf3",
            "fb636ef19f064c0da23b1c25b8055cc2",
            "b664e577a4224495b9ba1d3e9e9f086f",
            "86a0223ab5b646779c79368a9488f73e",
            "0fd9acedb3994aa18a736e9348b488be",
            "67fd1c64db224ddea027ab19af28ee7a",
            "6120b2beb966442095f6518224e4e4a9",
            "03d13bd5cd7d4e47b71db21b1433d4d5",
            "0e9239ce374c46e3b6b77247013ee0d6",
            "eabdb863fbad45e4aff91f16e705db4d",
            "7d73257a78c94b49a027f75e076b45a5",
            "9801779c6de64213ad98b6eb4d001424",
            "eeb3fcd8b6d4493aac72646cb8f4d69d",
            "19cd7b1d68354390b00503eacc80dea8",
            "fd252742ec194a6482355b5a6ba77dfe",
            "7e798b3cb4234eb3b99eb7405c612b0a",
            "65cfa6d672244ec28a0b69d3730116ef",
            "ceb8af85be3e4ef7b029d51b77bcf1d8",
            "d2330abc93fa4381a654ad1c2ee39e3e",
            "81a4e7c73eee4e39a93c626f95fff647"
          ]
        },
        "id": "OvFpi6WBl8SL",
        "outputId": "b8c9610e-8609-4239-fb58-0d1a6f9989d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bab4ed0d1ff747e5abc69996ae8bc1a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eabdb863fbad45e4aff91f16e705db4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization complete\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 36000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"Tokenization complete\")\n",
        "print(tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aUUS8YKmC-U",
        "outputId": "c1874269-a536-41bd-9706-8adf822561d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization complete\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 36000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        add_special_tokens=False\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"Tokenization complete\")\n",
        "print(tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kByK4jUcnQGL",
        "outputId": "5ba5a453-f627-4152-9baa-db553b800811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attention_mask removed\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids'],\n",
            "        num_rows: 36000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tokenized = tokenized.remove_columns(\"attention_mask\")\n",
        "\n",
        "print(\"attention_mask removed\")\n",
        "print(tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "ded82c3dfe564d889f38eee9f747fbaa",
            "94698844e9c940d99b3d4c8f2cfd86e8",
            "3b67dc8d4cc04811861e4e6c7f5f3239",
            "3589e85af35847d8b17b75d12d586339",
            "5595a94ffeb149499148ff9d479723c5",
            "00cf20fe7ec541cab85c143b080ce2f6",
            "ab7d1ed9950547088c69809500800fed",
            "17bd963ba0e1440aa2c0e44de3e1e1df",
            "bd6e5ddec57e40a1b5a1e7a20293016f",
            "a1aed93d5b284e1ebb6ff23a06602560",
            "aec93fc3d3e140b48f8cef1d05cd413c",
            "89808949ef6e4f919ac4d41956950dd0",
            "6b73b516164240d8958020362bb3e6ec",
            "9b048c950b7b42fba3a010d51bd9b69d",
            "c485add5432a4d35999511e407606dc8",
            "2f9f6b71fd8a48ca92929d804fa2a1ef",
            "44149c69aab944b4b8b3df45be63e64a",
            "2a11aed47f574c2e96d71f52b065a3be",
            "9a6a7bd78e6748c1b30e4cd9f5f398d4",
            "1eaace267ec0437398b108ac4349df12",
            "9c8d2f7ef8dd428382af789733ed8e13",
            "ab452c48dbb6407b9f17df15ef817e7f"
          ]
        },
        "id": "izM6CYVvnM8h",
        "outputId": "61c6a7e4-4238-4e98-956f-9b1b1599543f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded82c3dfe564d889f38eee9f747fbaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89808949ef6e4f919ac4d41956950dd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lm_dataset created\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 1029\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 115\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "block_size = 256\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = sum(examples[\"input_ids\"], [])\n",
        "    total_length = (len(concatenated) // block_size) * block_size\n",
        "\n",
        "    input_ids = [\n",
        "        concatenated[i:i + block_size]\n",
        "        for i in range(0, total_length, block_size)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": input_ids.copy()\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "print(\"lm_dataset created\")\n",
        "print(lm_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8enFH5RmFzk",
        "outputId": "04e7e6cf-c716-4682-b4af-d6b3e64b7a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoader ready\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    lm_dataset[\"train\"],\n",
        "    batch_size=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"DataLoader ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "138f35ecff1b447e8578416e4fc29617",
            "2d0a275cfcb2417c931284db96aa4c21",
            "0008d4a9a8eb4d648e5829a0aa1c4f75",
            "580397794c714c6e99c606517f1b8d61",
            "9370157a11ad4ea1a69f4474f1690301",
            "66430bc2b3b94d438cfacb5ce3c57fc8",
            "f1fbc3718ab54cc092af4fd4aed2de43",
            "4d3dd01f7f674961b593cd1c957b7a6e",
            "de54bb7a0e564c35bccb1816672cbc79",
            "07a6b5a1a8ae458d975820afa95dab68",
            "420ddd441d7149e9a2f73ac33872af10",
            "adeb0bb89e1b4aa29545380f182caec0",
            "459aea1091d24965b06709b0a42ff91e",
            "f3cd680ca0f045d8a7ec8b03142945b0",
            "3156e0a285ea4c90977877d21a8f83bd",
            "c7cce3b6de4e47e484422ce6a416b1e0",
            "6190ba6ec9dc41ec98b47954c07ef4b3",
            "e4deec8d0bc140f7b71043788eb71e94",
            "6f604d0653554332a6a9c6ae1cb9eb80",
            "9a4f3357602342a9ab9f2b3855cc658e",
            "d2995637e5bd41109df02ad81dc3e854",
            "4e1a1d0f31494b4789f1a1a2f242c4eb"
          ]
        },
        "id": "XrdDmSGxmJ9s",
        "outputId": "9069f757-0de2-49ca-d0f1-f82b6c43547f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "138f35ecff1b447e8578416e4fc29617",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adeb0bb89e1b4aa29545380f182caec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT-2 loaded\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "print(\" GPT-2 loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojvYc_NQmNXP",
        "outputId": "de74c47f-36b2-421a-d2f3-94922a2243f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LoRA config created\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\"],  # GPT-2 attention\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "print(\" LoRA config created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkhejTs-natl",
        "outputId": "9a487dc3-0593-46eb-bd0c-b6c1dffeaacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LoRA applied\n",
            "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\" LoRA applied\")\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHqgqn4anfZ8",
        "outputId": "4bb82f5a-f204-4c0e-e092-ddf71bc2f284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer & device ready: cuda\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(\"Optimizer & device ready:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTaLdyzQoIv8"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n",
        "    labels = torch.tensor([item[\"labels\"] for item in batch])\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWLWgKyqoMi8",
        "outputId": "4805e9ca-a0f9-4f56-9af6-690e9ebb5d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoader ready with collate_fn\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    lm_dataset[\"train\"],\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"DataLoader ready with collate_fn\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rHjkvTRnlWx",
        "outputId": "ba75a66b-c694-485d-fb1c-7e943ac8a4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd01 Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/258 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:46<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average loss: 4.9814\n",
            "\n",
            "\ud83d\udd01 Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:47<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average loss: 4.7633\n",
            "\n",
            "\ud83d\udd01 Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:50<00:00,  5.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Average loss: 4.6806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 3\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n\ud83d\udd01 Epoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\" Average loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8SKSYZgnsB3",
        "outputId": "5475bdec-3b03-48d9-a2bf-fd69f63db1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LoRA adapters saved (few MB)\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"lora-shakespeare\")\n",
        "tokenizer.save_pretrained(\"lora-shakespeare\")\n",
        "\n",
        "print(\" LoRA adapters saved (few MB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09E_h3rUntg-",
        "outputId": "7b5b46d6-306b-4f6a-b2cd-795e229c73b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GENERATED TEXT\n",
            "Hello I am Reading your letter,And I will tell you your name, but I will tell you your name.I am, in your own words, the one that will take my life:By-come, come, let me go.What is this?The truth, the truth, the truth!Now what are you?You say that I may find her, but I am notA true soldier, but a soldier who is not.I have given a command to thee.But yet you know it, thou art not to be found with my word:Is it true that you are a man?No\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"Hello I am Reading\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        temperature=0.8,\n",
        "        do_sample=True,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "print(\"GENERATED TEXT\")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "699f27d4d80841aa99626faa6b11e5d7",
            "7c0ac54de9b64404a0fd2342d4b86d00",
            "2b1839da352f417289414e3517ac7231",
            "fcb59e1337a54633ab7316f2308b2256",
            "1d48768b2d514bcebfeb7a687bf77e83",
            "88e5e267da764011b253155b661a7fe7",
            "6ae5e4c9d8794197b95dab13bfd9ddd8",
            "b3c30deac67c495baca0a4f3f08af403",
            "3733b77f292243deb22d67a67baf76ea",
            "13f03b632fb3432aa1d36f1f9af35473",
            "ac5f296a37284452af18015747d34dc3",
            "985a5456532e4ee9aa4f0326fff12f48",
            "4868e539731b4c75aed7a51ba75a406f",
            "2e9f6083fc4742a78a75f72c16a96dde",
            "8e684a1050c9474690599b3d0119c0fa",
            "110263006b304c9f94dfdea04ff730c1",
            "f869305479e248e1929e1321bd0dbb03",
            "42bda97aa42e48918d76e7700f07f0c6",
            "cb28ea0d28e749369e54d4100a9a3064",
            "664bad06311d4df88be54e3d1994cc46",
            "6d06b0cd9bf2445682b0c6b56524c3cc",
            "e9fdacfb0b4546da86bd81428fd526c8",
            "deddda0b74104e5688f46e7ba7f8f916",
            "ef5209b3f0dc4c8bad53877ea4bf2ad9",
            "924fa1f69e22413fa06fdc330c32e926",
            "7d25bb4732c14624a365348f32e1cb2f",
            "790548d211b64e1c9cad31f77e3b7416",
            "fd37438af8dc454c9ae1fbc13816703e",
            "cd17f11d4b51459daba0ebd6a4327c20",
            "3d1f2f1b54a84559a740702bfe5cfbe3",
            "b9b79b71a8f64e349fcfb349bfaf8962",
            "2ee18485aff54433b35923ef5d4c8f15",
            "373edcde1b494b249c92f2af83b5dddf",
            "0c618440e9724688aaf9d5fecc983534",
            "a52b1edeed9948a88e8d9a29a9c0dffe",
            "cfc6f44ed2e149d68b714e30d6f6db66",
            "83b012a6cc8c42a485251ba02eae53f6",
            "4bc70dbed07a444e847f74d1b21afb72",
            "22faaaf21d0d4f67a8e205872ae9b92f",
            "c8e2b02cdc3b4e8f8a95e973db1b4d62",
            "92a64886f429486b92772528eeafd445",
            "525e580fe4cc43b2b927aa502e5817e7",
            "e3889fada8ea46358b2b3a2cf0f7e6c6",
            "4e0377517eb2426dade4b43fb4af1fe6",
            "4129459df85246559479c72517388e79",
            "be1d2e913afd4159b34ffbd654c1c407",
            "aceb1e74954749e584bc669e0f3098a8",
            "1db527a6efa840a9adb82060146febd8",
            "3f79913ff51f4f858fddaf9f302437ba",
            "70827ffd76344ecc8d7dbaf0fac65f0d",
            "52b1847eff594bd38dd48703463d5cc2",
            "720e196875fd44969f7e0314800390e5",
            "3eff44741f35431081594713d5045fc6",
            "e32c8cb3c237409ab8869a00f8ae57a4",
            "298cbb5233a84209b69660adfb22c6c4",
            "4f452c990af9407ea9196616c63c4847",
            "aefc2b7b14de4eda9facd5b0f98af13b",
            "2d3c0c5007fa4952a4c7ef116c0fd20a",
            "a8c3ddbe511b403896b3ba77a06b024f",
            "6b1c4b4887864ca184c6b4485d695841",
            "56ff063aa41e44a0a8139b89018e38ee",
            "1bc58715ca5e40febc28113bde34010b",
            "f7a8e4070dab46c58f7b65af2f96bbe0",
            "c14911bdef17458f9ef2a357836b644d",
            "b681349053c0459cbf2070b84a5ebd86",
            "8a0ee5dcc3bc4cc0a440e0e358a4f1f1",
            "3a1100740341423cb81e2bab726d78c0",
            "1be4a9e94c53469e820086fdc015a4f0",
            "c7426b3b469141da984c95ca5e1478bd",
            "09b645f20e254764bc42d6e994170d97",
            "44831d1d494b40af942f6d40b5f66e2b",
            "e7f1107d07ee4c6a9997da5711b9cdfa",
            "f1d2e0a9381542b19e91169801fb2f5d",
            "f7b27e0e0bb14c03a7cec8208f3fd7fb",
            "d814d956c50d47809c0809a05889bf30",
            "91ceb2f6fb8e4e0ba59daa9eaed54e50",
            "c6218957a7d8488fa1c9bd358087a45d",
            "1868e5382af04b288f6271660fd7bc21",
            "80f9275261cd40aeabdc8aaf6547a392",
            "6a613acb58204bd7be01f7e4fa752c1d",
            "0abe329318cf48019657a4d1879a8347",
            "278b1274285d4e8888114e9113bb642a",
            "744a6c20e654475b8135d8f7471fb585",
            "e2927854ccfd48d4bfcd4cb0205d813a",
            "7f065bd4c5f94d9c95d7c5050b7891ab",
            "c38cb88e60df468da4e99a39f73ce913",
            "eef4aff2a0174b1f972fdf09bc10bb04",
            "69bf173e76124d679896641dd00a9b5c",
            "ea6f45a018b14018b8fb9a47fee30f27",
            "2f41b7e170504cb194cddbe4119e9b37",
            "5145bb524769444c80e55bc1ee0345b4",
            "549c2c5c49e24bfa89a83db613773ace",
            "173960eefb8245bea880f616567d0687",
            "a14568b40d9c47da90f9eb78d963b965",
            "19f9ee4b881b42a099617d5c386fafb1",
            "28dfba997d2e4363a279cbfee64c9760",
            "0e3ff11b63574db2a697b788c18ef092",
            "d01a4435e87546958169df5c4ff86d1d",
            "0d7539ed159d4e9789d749af1bfd605a",
            "42382422a4f845e9bef8efa2a9136659",
            "fb88ab52ca3b4e949c46246dc0c64a7c",
            "0cbe0e9cfe0145abb40f96adcc64c923",
            "43268aacc5954ddf8ed93e53e2235b3e",
            "358114fb9f1a4923b8f85f47a0b45b0f",
            "a20310488dbd4e2d8056391a018607e2",
            "1700a5c67d3a4b1790d3e931991b5f02",
            "6216b345812b4d0180e31b1e3163864d",
            "a07078b0259b4181a50a87ea8d5275ac",
            "22263a86d1e64287880e60db7bfeb79e",
            "6a9152996b4a43b4b787303f9f45259b",
            "810cf7afa130491da06b3549e82c2d6d",
            "abc829d5d7f44a6b9b74e15dd3e1f431",
            "d3f03c379ab34e83bf363c7159db291c",
            "7d52ed7835e1486c8b26cccecbf8b16b",
            "33f817a34ea14b5bbdb10d7c0521b398",
            "2b26b45c98cc47e39a905505a7bdea0d",
            "a7dc615c59e4424cbc31df964f05f533",
            "badfe27a7b7f49ccb3a293908b195694",
            "687c1360ab9d4deb9682b44bb4d3e912",
            "dccdef7e99a84121966cbf488752e6fd",
            "f23ef1cd58dd4f30bd7b6f27a718493f",
            "c6439cb7d9194d249d8ba6a6c1a6408a",
            "370e5f7ae7cb414188bb9f96bcae05bc",
            "9b317ac3e7a147b7a4451e1f44c8f788",
            "317bf449be4f469da5c7429041ee71f1",
            "487e5d75c9264dbeb5bb3b611502e601",
            "b5cf2a573be341aba6f90f234df61d7a",
            "1786f4f34328446e8344fa5260562988",
            "e8f9d43f169b48a495a505c12a357e67",
            "ad695e829b1a48e3985f631953dfeaa6",
            "5cbcb72111134a2c8dc68843ff9c9ece",
            "b754beede8bf43d38028c85cf8346c82",
            "0c0d53f646c44b2dafb888e5a2cfd7fa",
            "9a5e8d846ba24ca8bafd26681c994dd6",
            "56505ab4a8814c978b3c23b3f89a2bb8",
            "a4465b94a1d7459da55335f0a4961c56",
            "9aa3762598e24ee6b15c60bf4d5033b6",
            "d3b18c2c66dd49b6829fb53fa7b212ab",
            "603eb7c0916a49e98e9ea26a746cdb82",
            "e581c0774f2e489aab8a941b1f96c043",
            "2219f5c9d7bd4f41a488fa7ce6fe76e4",
            "b26ad2a1d7d941079d2c919d8b8e202a",
            "180f905f2e3248c9b9d1a9cacfdc5821"
          ]
        },
        "id": "TlcNKYYQ91XK",
        "outputId": "ae371913-0eba-45be-fe58-8722e53e12a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "699f27d4d80841aa99626faa6b11e5d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "985a5456532e4ee9aa4f0326fff12f48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deddda0b74104e5688f46e7ba7f8f916",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c618440e9724688aaf9d5fecc983534",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4129459df85246559479c72517388e79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f452c990af9407ea9196616c63c4847",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a1100740341423cb81e2bab726d78c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1868e5382af04b288f6271660fd7bc21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea6f45a018b14018b8fb9a47fee30f27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42382422a4f845e9bef8efa2a9136659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "810cf7afa130491da06b3549e82c2d6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION GPT-2 (Before Finetuning)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6439cb7d9194d249d8ba6a6c1a6408a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0d53f646c44b2dafb888e5a2cfd7fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 5.2251\n",
            "Perplexity: 185.88\n",
            "GENERATION SAMPLES - BEFORE FINE-TUNING\n",
            "\n",
            "Prompt: 'To be or not to be'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: To be or not to be, this is a great way to do it.\n",
            "\n",
            "In addition to the great value it has, it also comes with an extra $30. You get $10 off your purchase.\n",
            "\n",
            "And if you are looking for an instant \"buy it now\" deal, it is also available on Amazon.\n",
            "\n",
            "Here is a video to help you get started:\n",
            "\n",
            "We hope this helps\n",
            "\n",
            "Prompt: 'Once upon a time'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: Once upon a time, it would have been good to have some sort of political change that would be able to keep the regime from falling apart. But now the whole world has turned against us and our interests are at stake. I can't believe how stupid we have been, how ridiculous we have been. I know what you mean by \"unacceptable\". I know what you mean by \"unacceptable\". It's not\n",
            "\n",
            "Prompt: 'The king said'\n",
            "Output: The king said to him, \"The men of the city are very strong; there is no other way.\"\n",
            "\n",
            "The king said, \"You shall not leave your people to the wolves. It is your duty to put an end to them; do not allow them to come out of your way.\"\n",
            "\n",
            "The king said, \"Do not be afraid; we will be with you, and it will be\n",
            "\n",
            " evaluation complete!\n",
            "Save these metrics: Loss=5.2251, Perplexity=185.88\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "block_size = 256\n",
        "batch_size = 4\n",
        "\n",
        "# load shakespeare\n",
        "dataset = load_dataset(\"text\", data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    all_masks = sum(examples[\"attention_mask\"], [])\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"attention_mask\": [all_masks[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"labels\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
        "    }\n",
        "\n",
        "test_loader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "def calc_perplexity(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "            mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "            labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            total_loss += out.loss.item() * ids.size(0)\n",
        "            count += ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    return math.exp(avg_loss), avg_loss\n",
        "\n",
        "# load original gpt2\n",
        "\n",
        "print(\"EVALUATION GPT-2 (Before Finetuning)\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "base_ppl, base_loss = calc_perplexity(model, test_loader)\n",
        "\n",
        "print(f\"\\nTest Loss: {base_loss:.4f}\")\n",
        "print(f\"Perplexity: {base_ppl:.2f}\")\n",
        "\n",
        "# generation samples\n",
        "print(\"GENERATION SAMPLES - BEFORE FINE-TUNING\")\n",
        "prompts = [\"To be or not to be\", \"Once upon a time\", \"The king said\"]\n",
        "for p in prompts:\n",
        "    print(f\"\\nPrompt: '{p}'\")\n",
        "    inp = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inp, max_new_tokens=80, temperature=0.8, do_sample=True, top_p=0.9)\n",
        "\n",
        "    print(f\"Output: {tokenizer.decode(out[0], skip_special_tokens=True)}\")\n",
        "\n",
        "\n",
        "print(\"\\n evaluation complete!\")\n",
        "print(f\"Save these metrics: Loss={base_loss:.4f}, Perplexity={base_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7ef70e96f1747f2aa183ed19eae18f8",
            "dfa0a4930cbc4e2ba6f85cd5dc704766",
            "dd753a279d8148c29926b6c7be6cc04d",
            "ce95a12d81f3410ebfe5e35ffeea2ede",
            "2006b53a8b514ca196633d09be3d2618",
            "2c087564b4c64b0d804c6df2ab495e96",
            "c85fb7aa604749e0acaecf1a3c14d502",
            "3518b19f1b2d4c3f8e2380ae123fe152",
            "a807f64d842643efa51bb6bac2782c92",
            "e35264001fe84573a8bea50d0dfd0daa",
            "918db613fbe449f3a159d2a9f704df74",
            "7874c3e7c8b04882a8e5eb12cafb96de",
            "2fefd68217f145f0bf45edd218dc5d97",
            "2da4a3fcb82c4b4ab1cddfd2b4e8989a",
            "6bbbea4fd9b64f53a096f77f1f636563",
            "97f72d19751649e2b7196f32ad8571a3",
            "73510a1a9eda42dc8538f8d93bd707aa",
            "f2d6f2a7c2d94715a8217a0c45ea9a92",
            "959661dc193649809f9c5322c58fd876",
            "1a0bf0e4682048489580b4885814d793",
            "7d62948d52f54e5a82eeff2131fcc37b",
            "6c2a08506a8d4d719f8cc76673994196",
            "58ce37b3005c4b04b3141f213a54473a",
            "20a23995afdd4e51b679193c35a98d84",
            "4c774b6c9f464515b20df10bf694cbb9",
            "9b0d17c304e54a148295566e30dfbe2c",
            "d5b594d0f34347e3838b50f0f0e5f6b4",
            "9dd9d8304fa94984ad56fff50b11c83d",
            "8e4c689bb2a6434cbf9469a7ca284fee",
            "ba10fb796a6f4f9292aeb81bd164405d",
            "9eb794899f38423780c1eb8ce8ec1105",
            "d307e6443fd64af2a3d32c8f893f02af",
            "7207ce61474141549f392cdeba9e0595",
            "0750c7c99a4b471d9e28602bd7a1a9d8",
            "35da550833114f0e9ecd056228ffb562",
            "9a470fb2ff034593b95021a8f286ea80",
            "d336ce771cd6454c9ba19713de61b8aa",
            "e80d5f6e74404b0bb18236f05e0512ac",
            "d997eeb2e6444e408afb64022f432814",
            "b7a4b427ccc743e48369f95b413267e4",
            "aeff23e56ca94e8193fe032e20da0cc0",
            "4cb1aa4c13c242b89f810bd520d2fe52",
            "1b97e327f14d4a3fb8ba34cdff06cf73",
            "9095fa0cb9fa4c14857401e00a8473ef"
          ]
        },
        "id": "3xNiAHThh_rx",
        "outputId": "c754c829-bf86-4aec-eeb1-77ded2b9b254"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7ef70e96f1747f2aa183ed19eae18f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7874c3e7c8b04882a8e5eb12cafb96de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58ce37b3005c4b04b3141f213a54473a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0750c7c99a4b471d9e28602bd7a1a9d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n",
            "\n",
            "FINE-TUNING GPT-2 WITH LORA\n",
            "\n",
            "epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:51<00:00,  5.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 5.0204\n",
            "\n",
            "epoch 2/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:53<00:00,  4.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.7976\n",
            "\n",
            "epoch 3/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.6966\n",
            "\n",
            "epoch 4/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.6358\n",
            "\n",
            "epoch 5/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.5894\n",
            "\n",
            "epoch 6/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.5579\n",
            "\n",
            "epoch 7/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.5306\n",
            "\n",
            "epoch 8/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.5066\n",
            "\n",
            "epoch 9/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4886\n",
            "\n",
            "epoch 10/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4709\n",
            "\n",
            "epoch 11/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4547\n",
            "\n",
            "epoch 12/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4379\n",
            "\n",
            "epoch 13/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4243\n",
            "\n",
            "epoch 14/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4103\n",
            "\n",
            "epoch 15/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 258/258 [00:54<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg training loss: 4.4012\n",
            "\n",
            "merging lora with base model...\n",
            "\n",
            " Fine-tuning complete! Model saved to gpt2-lora-merged\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_2c34f20d-38ad-4bd7-911c-51760f9ac45b\", \"gpt2-lora-merged.zip\", 463867415)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "block_size = 256\n",
        "batch_size = 4\n",
        "epochs = 15\n",
        "lr = 1e-4\n",
        "\n",
        "adapter_path = \"gpt2-lora-adapters\"\n",
        "merged_path = \"gpt2-lora-merged\"\n",
        "best_checkpoint = \"best_model_checkpoint\"\n",
        "\n",
        "# load shakespeare\n",
        "dataset = load_dataset(\"text\", data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    all_masks = sum(examples[\"attention_mask\"], [])\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"attention_mask\": [all_masks[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"labels\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(lm_dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# load gpt2 and add lora\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.15,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# training\n",
        "print(f\"\\nFINE-TUNING GPT-2 WITH LORA\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nepoch {epoch+1}/{epochs}\")\n",
        "    running_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        loss = out.loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"avg training loss: {avg_loss:.4f}\")\n",
        "\n",
        "# save\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(\"\\nmerging lora with base model...\")\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "model = PeftModel.from_pretrained(base, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "model.save_pretrained(merged_path)\n",
        "tokenizer.save_pretrained(merged_path)\n",
        "\n",
        "print(f\"\\n Fine-tuning complete! Model saved to {merged_path}\")\n",
        "\n",
        "# download\n",
        "shutil.make_archive(merged_path, \"zip\", merged_path)\n",
        "files.download(f\"{merged_path}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# check if model exists\n",
        "if os.path.exists(\"gpt2-lora-merged\"):\n",
        "    print(\"\u2713 Model found!\")\n",
        "    print(\"\\nFiles in the model directory:\")\n",
        "    print(os.listdir(\"gpt2-lora-merged\"))\n",
        "else:\n",
        "    print(\"\u2717 Model not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFNIXsESpFdp",
        "outputId": "7c4765a7-6a2c-4a80-8206-d72a775c93d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Model found!\n",
            "\n",
            "Files in the model directory:\n",
            "['merges.txt', 'special_tokens_map.json', 'vocab.json', 'model.safetensors', 'generation_config.json', 'config.json', 'tokenizer.json', 'tokenizer_config.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "model_name = \"gpt2\"\n",
        "finetuned_path = \"gpt2-lora-merged\"  # path to your fine-tuned model\n",
        "block_size = 256\n",
        "batch_size = 4\n",
        "\n",
        "# baseline metrics from CODE 1 (update these with your actual values)\n",
        "baseline_loss = 5.2251\n",
        "baseline_ppl = 185.88\n",
        "\n",
        "# load shakespeare\n",
        "dataset = load_dataset(\"text\", data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    all_masks = sum(examples[\"attention_mask\"], [])\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"attention_mask\": [all_masks[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"labels\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
        "    }\n",
        "\n",
        "test_loader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "def calc_perplexity(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "            mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "            labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            total_loss += out.loss.item() * ids.size(0)\n",
        "            count += ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    return math.exp(avg_loss), avg_loss\n",
        "\n",
        "# load fine-tuned model\n",
        "\n",
        "print(\"EVALUATION - GPT-2 (After Fine-tuning)\")\n",
        "\n",
        "\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_path).to(\"cuda\")\n",
        "ft_ppl, ft_loss = calc_perplexity(finetuned_model, test_loader)\n",
        "\n",
        "print(f\"\\nTest Loss: {ft_loss:.4f}\")\n",
        "print(f\"Perplexity: {ft_ppl:.2f}\")\n",
        "\n",
        "# comparison\n",
        "\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "\n",
        "\n",
        "loss_change = ((baseline_loss - ft_loss) / baseline_loss) * 100\n",
        "ppl_change = ((baseline_ppl - ft_ppl) / baseline_ppl) * 100\n",
        "\n",
        "print(f\"\\nMetric Before After Change\")\n",
        "\n",
        "print(f\"Loss:           {baseline_loss:.4f}      {ft_loss:.4f}      {loss_change:+.1f}%\")\n",
        "print(f\"Perplexity:     {baseline_ppl:.2f}     {ft_ppl:.2f}      {ppl_change:+.1f}%\")\n",
        "\n",
        "if ppl_change > 0:\n",
        "    print(f\"\\n Model improved! Perplexity decreased by {ppl_change:.1f}%\")\n",
        "else:\n",
        "    print(f\"\\n Model got worse. Perplexity increased by {abs(ppl_change):.1f}%\")\n",
        "\n",
        "# generation comparison\n",
        "\n",
        "print(\"GENERATION COMPARISON\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "prompts = [\"To be or not to be\", \"Once upon a time\", \"The king said\"]\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nPrompt: '{p}'\")\n",
        "    inp = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # before\n",
        "    with torch.no_grad():\n",
        "        base_out = base_model.generate(**inp, max_new_tokens=80, temperature=0.8, do_sample=True, top_p=0.9)\n",
        "    print(f\"BEFORE: {tokenizer.decode(base_out[0], skip_special_tokens=True)}\")\n",
        "\n",
        "    # after\n",
        "    with torch.no_grad():\n",
        "        ft_out = finetuned_model.generate(**inp, max_new_tokens=80, temperature=0.8, do_sample=True, top_p=0.9)\n",
        "    print(f\"AFTER:  {tokenizer.decode(ft_out[0], skip_special_tokens=True)}\")\n",
        "print(\"Evaluation complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754,
          "referenced_widgets": [
            "1a18162b797149f0b4ab450b997651b3",
            "9899ac71b97a48e7baa625a83fed1ed7",
            "c55d2dd2cd234e50bfe0382fb2a96c73",
            "0846af2b071a4f2ab37055dbb1006569",
            "00334d837c5a4a20ad71f4a3a688774d",
            "0307ef2638c44d7d86eeb2857a5de904",
            "61879a30d6a846999b2f032492a4e7bf",
            "76663e3f384344c1b4a40c4000420fa5",
            "9bd36acf9fe04b5a809e5eb52cf81142",
            "93ed7b45e579490ebe1c0394a0ae14cc",
            "ce528470089446228c5804332fe5b961",
            "54143b74eb5043739dc0910bf024c41f",
            "158d44ecfa1040f3bf83475bea14fbee",
            "e95078c22c85484bb628f506ce6e8e8d",
            "e817484de7b844a994136d4a1cfd883e",
            "59e7b4e6839d4de7995c8a0efd0a7f67",
            "d18b61e4e60f492eb21c2ba257be79bf",
            "e4d24949a85d4791b7520e980119e6c1",
            "a90ed162570940c684d6c3de7e15829d",
            "3a1e20b2f99f4ad08a91e509dd85142e",
            "a5878a3d47fb4bcdb73788b3f64dbf61",
            "b122d68ea0244bc7b4d2a109e672ec3d",
            "bddb5b00bf3d4a55b5247adfcd511045",
            "084047f3d6084df1ae286a11dde1ca43",
            "162791f66b8b46d69d6de29052e182c0",
            "c677d458131c49d693cdc2c3d5288af3",
            "37d624b4c71d47b0873087030ee3fabe",
            "b6a41b8612524518993d21cb904344e7",
            "f2f854a288b24dff9a93cdb589b0b940",
            "e3d6bab437f243cbb550cb8004293651",
            "8aadebb5b3234058a388205e96438163",
            "1c1f0ea1dac94c7dacc87d2a4abdfe2e",
            "4c245ff9c623409394742e17ad0dcd5f",
            "4cef249332ed405c82be01f159a630b9",
            "6c0cad5c29d44fda978ed978ddf0e765",
            "fa26ce6ccd3244c7b618d4d75d8752d5",
            "3d80fd5ada6a4e86af779003b4e2c5ad",
            "beb5f47cca4148efafb7a84ef8690ed3",
            "6777de17bfc04f9eb0fba888edf2820b",
            "bc30ad0ff76f4d7ba9766e0161ff72d8",
            "9205cde60baa4d9fa6e557b3e0ab39e6",
            "3c8c6abba5e042bbb3941c3707f6b931",
            "9feff0099a91414991681a0cb0a5cc04",
            "e4561f52b4ce4a138fcb7923b42830c7"
          ]
        },
        "id": "sbsSfhfDn0Mj",
        "outputId": "ba9d1afc-8e9e-4027-99f8-a65ca4f4ba62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a18162b797149f0b4ab450b997651b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54143b74eb5043739dc0910bf024c41f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bddb5b00bf3d4a55b5247adfcd511045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cef249332ed405c82be01f159a630b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION - GPT-2 (After Fine-tuning)\n",
            "\n",
            "Test Loss: 4.3850\n",
            "Perplexity: 80.24\n",
            "PERFORMANCE COMPARISON\n",
            "\n",
            "Metric Before After Change\n",
            "Loss:           5.2251      4.3850      +16.1%\n",
            "Perplexity:     185.88     80.24      +56.8%\n",
            "\n",
            " Model improved! Perplexity decreased by 56.8%\n",
            "GENERATION COMPARISON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'To be or not to be'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE: To be or not to be, if the individual is an atheist, he or she is not a member of the family or the church. A person who is an atheist may not be a member of the family or church. He or she must be married or a parent, parent, child, sibling or spouse of the person.\n",
            "\n",
            "A person who is an atheist may not be a member of the family or church. He\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFTER:  To be or not to be, to be.What we are doing, we have done; but to be not.What, you say? and so you, that be in a hurry, I pray,You are not a thing to be afraid of.I am not so; and I'll have you tell me.GLOUCESTER:And with all his powers I will not do it.Away, sir\n",
            "\n",
            "Prompt: 'Once upon a time'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE: Once upon a time the two factions met, the First, Second and Third Kingdoms were divided into two different governments. The First Kingdom, led by King Jarl Greymane, sought to secure the kingdom of Jarlswood, and the Second, Third and Fourth Kingdoms sought to secure the kingdom of Kingsport. Both governments were in a war of attrition between the two kingdoms, and the Kingsport Rebellion broke out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFTER:  Once upon a time of war, I had to stay;I'll have you here, and so he shall be.But I am in haste to find my cause.I'll give you all your time, and your leave,But I have got your house to yourself.And then the land is gone, and I shall be gone.And how he hath made so many of them,KING RICHARD III:\n",
            "\n",
            "Prompt: 'The king said'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE: The king said that if I had not become an apostle of the church, there would not be any place for me. He said that when I saw the King, I would not be able to come into the kingdom of God. And I said, What is it that I do not know? He said, That is, because I am not an apostle of the church. And I said, If I had not\n",
            "AFTER:  The king said, 'What then?'--My lord,--what is it?GLOUCESTER:ROMEO:LUCIO:And be sure to keep him from me.That, by my own choice, my lord, is but an idle man,The king of England, the king of Wales,KING RICHARD II:KING RICHARD II:KING RICHARD III:\n",
            "Evaluation complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}