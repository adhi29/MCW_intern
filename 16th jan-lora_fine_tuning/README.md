# LoRA Fine-tuning Experiments

This folder contains my work on fine-tuning GPT-2 using LoRA (Low-Rank Adaptation).

## Files

- **load_gpt_2.ipynb** - Loading and testing the base GPT-2 model
- **lora_tuning.ipynb** - Fine-tuning GPT-2 with LoRA technique

## What I Did

Started by loading the GPT-2 model to understand how it works. Then experimented with LoRA fine-tuning which is more efficient than full fine-tuning because it only updates a small number of parameters.

## Notes

- Used Google Colab for running these experiments
- LoRA helps reduce memory usage during training
- Still learning about the best hyperparameters to use

---
*Work done on 16th January*
