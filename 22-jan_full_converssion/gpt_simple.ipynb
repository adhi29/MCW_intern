{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6grJQKqIFeLN"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers onnx onnxruntime-gpu torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "1dFGcQEDFoeq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading GPT-2 model from Hugging Face...\")\n",
        "\n",
        "model_name = \"gpt2\"  # Small model: 124M parameters\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Parameters: ~124M\")\n",
        "\n",
        "# Prepare input\n",
        "PROMPT = \"Explain quantum computing\"\n",
        "SEQ_LEN = 32\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    max_length=SEQ_LEN,\n",
        "    truncation=True\n",
        ")\n",
        "input_ids = inputs.input_ids\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "print(f\"Input: '{PROMPT}'\")\n",
        "print(f\"Input shape: {input_ids.shape}\")\n",
        "print(\" Complete\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmIg430JFqA4",
        "outputId": "a0ae1a54-9214-43fc-f1da-72da0b03c277"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading GPT-2 model from Hugging Face...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: gpt2\n",
            "Parameters: ~124M\n",
            "Input: 'Explain quantum computing'\n",
            "Input shape: torch.Size([1, 32])\n",
            " Complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Measuring PyTorch inference time...\")\n",
        "\n",
        "# Warmup\n",
        "for _ in range(5):\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Benchmark\n",
        "pytorch_times = []\n",
        "for _ in range(20):\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        pytorch_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pytorch_times.append(time.time() - start)\n",
        "\n",
        "pytorch_logits = pytorch_output.logits\n",
        "pytorch_mean_time = np.mean(pytorch_times) * 1000\n",
        "pytorch_std_time = np.std(pytorch_times) * 1000\n",
        "\n",
        "print(f\"PyTorch inference time: {pytorch_mean_time:.2f} ms ± {pytorch_std_time:.2f} ms\")\n",
        "print(f\"Output shape: {pytorch_logits.shape}\")\n",
        "print(\"Complete\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRFoI0XjFt_B",
        "outputId": "952dec81-6a4d-4de6-db69-e32021a36a35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Measuring PyTorch inference time...\n",
            "PyTorch inference time: 354.79 ms ± 115.15 ms\n",
            "Output shape: torch.Size([1, 32, 50257])\n",
            "Complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers onnx onnxruntime"
      ],
      "metadata": {
        "id": "a_XeF1OXF2Yi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "ixOX72YmF4NN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "model = model.cpu()\n"
      ],
      "metadata": {
        "id": "9iXJmcxPGcR8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2ONNXWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "        return outputs.logits\n"
      ],
      "metadata": {
        "id": "ckJ8EjxlGdu3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = GPT2ONNXWrapper(model)\n"
      ],
      "metadata": {
        "id": "5p_qUOfaGf32"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "SEQ_LEN = 16\n",
        "\n",
        "dummy_input_ids = torch.ones(\n",
        "    (BATCH_SIZE, SEQ_LEN),\n",
        "    dtype=torch.long\n",
        ")\n",
        "\n",
        "dummy_attention_mask = torch.ones(\n",
        "    (BATCH_SIZE, SEQ_LEN),\n",
        "    dtype=torch.long\n",
        ")\n"
      ],
      "metadata": {
        "id": "qn8VzITWGieR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q onnxscript\n"
      ],
      "metadata": {
        "id": "LQg3uq2ZGu5G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wbZVCPfDGun4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_path = \"gpt2_simple.onnx\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        onnx_model,\n",
        "        (dummy_input_ids, dummy_attention_mask),\n",
        "        onnx_path,\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"logits\"],\n",
        "        opset_version=14,\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "print(\"ONNX export completed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglQR85gGkXZ",
        "outputId": "f5c69085-4ba7-452a-abaa-30af4c04bba0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0121 10:49:36.376000 1914 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 14 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `GPT2ONNXWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `GPT2ONNXWrapper([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 14).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Translate the graph into ONNX... ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:Failed to convert the model to the target version 14 using the ONNX C API. The model was not modified\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
            "    converted_proto = _c_api_utils.call_onnx_api(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
            "    result = func(proto)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
            "    return onnx.version_converter.convert_version(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/version_converter.py\", line 39, in convert_version\n",
            "    converted_model_str = C.convert_version(model_str, target_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: /github/workspace/onnx/version_converter/adapters/no_previous_version.h:26: adapt: Assertion `false` failed: No Previous Version of LayerNormalization exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied 14 of general pattern rewrite rules.\n",
            "ONNX export completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_proto = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model_proto)\n",
        "\n",
        "model_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "print(f\" ONNX model verified\")\n",
        "print(f\" Model size: {model_size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO8YEBa8LEJ8",
        "outputId": "95581c69-81d9-490a-d497-f4ce46222982"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ONNX model verified\n",
            " Model size: 1.05 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, how are you today?\"\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN\n",
        ")\n",
        "\n",
        "input_ids = encoded[\"input_ids\"]\n",
        "attention_mask = encoded[\"attention_mask\"]\n"
      ],
      "metadata": {
        "id": "UwXXEooMLPfL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = ort.InferenceSession(\n",
        "    onnx_path,\n",
        "    providers=[\"CPUExecutionProvider\"]\n",
        ")\n",
        "\n",
        "onnx_inputs = {\n",
        "    \"input_ids\": input_ids.numpy(),\n",
        "    \"attention_mask\": attention_mask.numpy()\n",
        "}\n",
        "\n",
        "outputs = session.run(None, onnx_inputs)\n",
        "logits = outputs[0]\n",
        "\n",
        "print(\"ONNX inference successful\")\n",
        "print(\"Logits shape:\", logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw4KouoBLml4",
        "outputId": "b31be4a9-7da9-4b0c-f6e7-58e56e5be7ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX inference successful\n",
            "Logits shape: (1, 16, 50257)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Warm-up\n",
        "for _ in range(5):\n",
        "    _ = session.run(None, onnx_inputs)\n",
        "\n",
        "# Timing\n",
        "times = []\n",
        "for _ in range(20):\n",
        "    start = time.time()\n",
        "    _ = session.run(None, onnx_inputs)\n",
        "    times.append(time.time() - start)\n",
        "\n",
        "mean_time = np.mean(times) * 1000\n",
        "std_time = np.std(times) * 1000\n",
        "\n",
        "print(f\"Mean inference time: {mean_time:.2f} ms ± {std_time:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV8x7hUJLtdb",
        "outputId": "b141b958-5d54-490f-aa35-7e8150e98514"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean inference time: 77.01 ms ± 2.67 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2StaticKVONNX(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        past_key_0, past_value_0,\n",
        "        past_key_1, past_value_1,\n",
        "        past_key_2, past_value_2,\n",
        "        past_key_3, past_value_3,\n",
        "        past_key_4, past_value_4,\n",
        "        past_key_5, past_value_5,\n",
        "        past_key_6, past_value_6,\n",
        "        past_key_7, past_value_7,\n",
        "        past_key_8, past_value_8,\n",
        "        past_key_9, past_value_9,\n",
        "        past_key_10, past_value_10,\n",
        "        past_key_11, past_value_11,\n",
        "    ):\n",
        "        past = [\n",
        "            (past_key_0, past_value_0),\n",
        "            (past_key_1, past_value_1),\n",
        "            (past_key_2, past_value_2),\n",
        "            (past_key_3, past_value_3),\n",
        "            (past_key_4, past_value_4),\n",
        "            (past_key_5, past_value_5),\n",
        "            (past_key_6, past_value_6),\n",
        "            (past_key_7, past_value_7),\n",
        "            (past_key_8, past_value_8),\n",
        "            (past_key_9, past_value_9),\n",
        "            (past_key_10, past_value_10),\n",
        "            (past_key_11, past_value_11),\n",
        "        ]\n",
        "        past_key_values_tuple = tuple(past)\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            past_key_values=past_key_values_tuple,\n",
        "            use_cache=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        new_past = outputs.past_key_values\n",
        "\n",
        "        return (\n",
        "            logits,\n",
        "            new_past[0][0], new_past[0][1],\n",
        "            new_past[1][0], new_past[1][1],\n",
        "            new_past[2][0], new_past[2][1],\n",
        "            new_past[3][0], new_past[3][1],\n",
        "            new_past[4][0], new_past[4][1],\n",
        "            new_past[5][0], new_past[5][1],\n",
        "            new_past[6][0], new_past[6][1],\n",
        "            new_past[7][0], new_past[7][1],\n",
        "            new_past[8][0], new_past[8][1],\n",
        "            new_past[9][0], new_past[9][1],\n",
        "            new_past[10][0], new_past[10][1],\n",
        "            new_past[11][0], new_past[11][1],\n",
        "        )"
      ],
      "metadata": {
        "id": "pFNl_2wUMbVK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LAYERS = model.config.num_hidden_layers\n",
        "NUM_HEADS = model.config.num_attention_heads\n",
        "HEAD_DIM = model.config.hidden_size // model.config.num_attention_heads\n",
        "MAX_SEQ_LEN = SEQ_LEN # Using previously defined SEQ_LEN for the past KV length\n",
        "\n",
        "def make_past():\n",
        "    return torch.zeros(\n",
        "        (BATCH_SIZE, NUM_HEADS, MAX_SEQ_LEN, HEAD_DIM),\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "past = [make_past() for _ in range(NUM_LAYERS * 2)]"
      ],
      "metadata": {
        "id": "UisNCPMhMcvP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.ones((BATCH_SIZE, 1), dtype=torch.long)\n",
        "attention_mask = torch.ones((BATCH_SIZE, MAX_SEQ_LEN), dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "tqyLbXQ2Mi4-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_path = \"gpt2_static_kv.onnx\"\n",
        "\n",
        "inputs = [input_ids, attention_mask] + past\n",
        "input_names = [\"input_ids\", \"attention_mask\"] + [\n",
        "    f\"past_key_{i}\" if i % 2 == 0 else f\"past_value_{i//2}\"\n",
        "    for i in range(len(past))\n",
        "]\n",
        "\n",
        "output_names = [\"logits\"] + [\n",
        "    f\"present_key_{i}\" if i % 2 == 0 else f\"present_value_{i//2}\"\n",
        "    for i in range(len(past))\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        GPT2StaticKVONNX(model),\n",
        "        tuple(inputs),\n",
        "        onnx_path,\n",
        "        input_names=input_names,\n",
        "        output_names=output_names,\n",
        "        opset_version=18,\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "print(\"✅ Static KV cache ONNX exported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsTCAfyUMlcy",
        "outputId": "2b13dd7d-f880-4e37-9879-dae6b278414a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `GPT2StaticKVONNX([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `GPT2StaticKVONNX([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 14 of general pattern rewrite rules.\n",
            "✅ Static KV cache ONNX exported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75a59c8c",
        "outputId": "b5dfa7c2-dfac-471a-ee71-e654a954d36f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class GPT2StaticKVONNXInference:\n",
        "    def __init__(self, onnx_path, tokenizer, model_config, seq_len=SEQ_LEN):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_config = model_config\n",
        "        self.seq_len = seq_len\n",
        "        self.session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "        self.num_layers = model_config.num_hidden_layers\n",
        "        self.num_heads = model_config.num_attention_heads\n",
        "        self.head_dim = model_config.hidden_size // model_config.num_attention_heads\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=10):\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Tokenize the prompt without padding to get its actual length\n",
        "        initial_encoded_prompt = self.tokenizer(prompt, return_tensors=\"np\", truncation=True)\n",
        "        prompt_ids = initial_encoded_prompt.input_ids # Shape (1, actual_prompt_len)\n",
        "        actual_prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "        generated_ids = prompt_ids.tolist()[0]\n",
        "\n",
        "        # Initialize `current_past_key_values` and `current_attention_mask` dynamically.\n",
        "        # These represent the *actual* history accumulated so far.\n",
        "        current_past_key_values = [\n",
        "            np.zeros((BATCH_SIZE, self.num_heads, 0, self.head_dim), dtype=np.float32)\n",
        "            for _ in range(self.num_layers * 2)\n",
        "        ]\n",
        "        current_attention_mask = np.array([], dtype=np.int64).reshape(BATCH_SIZE, 0) # Initially empty\n",
        "\n",
        "        # Process prompt tokens one by one to build up the initial KV cache and attention mask\n",
        "        for token_idx in range(actual_prompt_len):\n",
        "            input_id_token = prompt_ids[:, token_idx:token_idx+1] # Shape (1, 1)\n",
        "\n",
        "            # Update the dynamic attention mask by concatenating a '1' for the current token\n",
        "            current_attention_mask = np.concatenate([current_attention_mask, np.array([[1]])], axis=-1)\n",
        "\n",
        "            # --- Prepare ONNX inputs, ensuring fixed shapes for the ONNX model inputs ---\n",
        "            onnx_input_ids = input_id_token # Already (1,1)\n",
        "\n",
        "            # Create ONNX-compatible attention mask by padding/truncating `current_attention_mask` to `self.seq_len`\n",
        "            onnx_attention_mask = np.zeros((BATCH_SIZE, self.seq_len), dtype=np.int64)\n",
        "            current_mask_len = current_attention_mask.shape[1]\n",
        "            if current_mask_len > self.seq_len:\n",
        "                # If current mask is longer than SEQ_LEN, take the last SEQ_LEN elements (sliding window)\n",
        "                onnx_attention_mask[:, :] = current_attention_mask[:, -self.seq_len:]\n",
        "            else:\n",
        "                # Pad with zeros at the beginning if shorter\n",
        "                onnx_attention_mask[:, self.seq_len - current_mask_len:] = current_attention_mask\n",
        "\n",
        "            # Create ONNX-compatible past_key_values by padding/truncating `current_past_key_values` to `self.seq_len`\n",
        "            onnx_past_key_values = []\n",
        "            for j in range(self.num_layers):\n",
        "                current_k = current_past_key_values[2*j]\n",
        "                current_v = current_past_key_values[2*j+1]\n",
        "\n",
        "                onnx_k = np.zeros((BATCH_SIZE, self.num_heads, self.seq_len, self.head_dim), dtype=np.float32)\n",
        "                onnx_v = np.zeros((BATCH_SIZE, self.num_heads, self.seq_len, self.head_dim), dtype=np.float32)\n",
        "\n",
        "                current_kv_len = current_k.shape[2]\n",
        "                if current_kv_len > self.seq_len:\n",
        "                    onnx_k[:, :, :, :] = current_k[:, :, -self.seq_len:, :]\n",
        "                    onnx_v[:, :, :, :] = current_v[:, :, -self.seq_len:, :]\n",
        "                else:\n",
        "                    onnx_k[:, :, self.seq_len - current_kv_len:, :] = current_k\n",
        "                    onnx_v[:, :, self.seq_len - current_kv_len:, :] = current_v\n",
        "\n",
        "                onnx_past_key_values.extend([onnx_k, onnx_v])\n",
        "\n",
        "            onnx_inputs = {\n",
        "                \"input_ids\": onnx_input_ids,\n",
        "                \"attention_mask\": onnx_attention_mask\n",
        "            }\n",
        "            for j in range(self.num_layers):\n",
        "                onnx_inputs[f\"past_key_{2*j}\"] = onnx_past_key_values[2*j]\n",
        "                onnx_inputs[f\"past_value_{j}\"] = onnx_past_key_values[2*j + 1]\n",
        "\n",
        "            outputs = self.session.run(None, onnx_inputs)\n",
        "            new_past_key_values = outputs[1:]\n",
        "\n",
        "            # Update `current_past_key_values` with the new output, these dynamically grow\n",
        "            current_past_key_values = new_past_key_values\n",
        "\n",
        "        # After processing the prompt, `current_past_key_values` holds the KV cache for the full prompt,\n",
        "        # and `current_attention_mask` is the mask for the prompt (actual length).\n",
        "\n",
        "        # Now, generate `max_new_tokens`.\n",
        "        # `input_ids` for the first generation step is the *last token processed from prompt*.\n",
        "        input_ids = onnx_input_ids # This is the last input_id_token from the loop\n",
        "\n",
        "        for i in range(max_new_tokens):\n",
        "            # --- Prepare ONNX inputs, adhering to fixed shapes ---\n",
        "            # Update the dynamic attention mask (for the *actual* history, then pad for ONNX input)\n",
        "            current_attention_mask = np.concatenate([current_attention_mask, np.array([[1]])], axis=-1)\n",
        "\n",
        "            onnx_input_ids = input_ids # (1,1)\n",
        "\n",
        "            onnx_attention_mask = np.zeros((BATCH_SIZE, self.seq_len), dtype=np.int64)\n",
        "            current_mask_len = current_attention_mask.shape[1]\n",
        "            if current_mask_len > self.seq_len:\n",
        "                onnx_attention_mask[:, :] = current_attention_mask[:, -self.seq_len:]\n",
        "            else:\n",
        "                onnx_attention_mask[:, self.seq_len - current_mask_len:] = current_attention_mask\n",
        "\n",
        "            onnx_past_key_values = []\n",
        "            for j in range(self.num_layers):\n",
        "                current_k = current_past_key_values[2*j]\n",
        "                current_v = current_past_key_values[2*j+1]\n",
        "\n",
        "                onnx_k = np.zeros((BATCH_SIZE, self.num_heads, self.seq_len, self.head_dim), dtype=np.float32)\n",
        "                onnx_v = np.zeros((BATCH_SIZE, self.num_heads, self.seq_len, self.head_dim), dtype=np.float32)\n",
        "\n",
        "                current_kv_len = current_k.shape[2]\n",
        "                if current_kv_len > self.seq_len:\n",
        "                    onnx_k[:, :, :, :] = current_k[:, :, -self.seq_len:, :]\n",
        "                    onnx_v[:, :, :, :] = current_v[:, :, -self.seq_len:, :]\n",
        "                else:\n",
        "                    onnx_k[:, :, self.seq_len - current_kv_len:, :] = current_k\n",
        "                    onnx_v[:, :, self.seq_len - current_kv_len:, :] = current_v\n",
        "\n",
        "                onnx_past_key_values.extend([onnx_k, onnx_v])\n",
        "\n",
        "            onnx_inputs = {\n",
        "                \"input_ids\": onnx_input_ids,\n",
        "                \"attention_mask\": onnx_attention_mask\n",
        "            }\n",
        "            for j in range(self.num_layers):\n",
        "                onnx_inputs[f\"past_key_{2*j}\"] = onnx_past_key_values[2*j]\n",
        "                onnx_inputs[f\"past_value_{j}\"] = onnx_past_key_values[2*j + 1]\n",
        "\n",
        "            outputs = self.session.run(None, onnx_inputs)\n",
        "            logits = outputs[0]\n",
        "            new_past_key_values = outputs[1:]\n",
        "\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = np.argmax(next_token_logits, axis=-1)\n",
        "\n",
        "            generated_ids.append(next_token[0].item())\n",
        "\n",
        "            if next_token[0].item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            input_ids = next_token[:, np.newaxis] # New token for next iteration\n",
        "            current_past_key_values = new_past_key_values # Accumulate KV cache\n",
        "\n",
        "        return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# Instantiate the inference class\n",
        "print(\"\\nInitializing ONNX Runtime inference...\")\n",
        "onnx_inference_model = GPT2StaticKVONNXInference(\n",
        "    onnx_path=\"gpt2_static_kv.onnx\",\n",
        "    tokenizer=tokenizer,\n",
        "    model_config=model.config,\n",
        "    seq_len=SEQ_LEN\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "initial_prompt = \"Explain quantum computing in a few sentences:\"\n",
        "print(f\"Generating text with ONNX model from prompt: '{initial_prompt}'\")\n",
        "start_time_generate = time.time()\n",
        "onnx_generated_text = onnx_inference_model.generate(\n",
        "    prompt=initial_prompt,\n",
        "    max_new_tokens=50\n",
        ")\n",
        "end_time_generate = time.time()\n",
        "print(f\"Generated text: {onnx_generated_text}\")\n",
        "print(f\"ONNX text generation time: {(end_time_generate - start_time_generate) * 1000:.2f} ms\")\n",
        "print(\"Complete\\n\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing ONNX Runtime inference...\n",
            "Generating text with ONNX model from prompt: 'Explain quantum computing in a few sentences:'\n",
            "Generated text: Explain quantum computing in a few sentences: Quantum computing in a few sentences: Quantum computing in a few sentences: Quantum computing in a few sentences sentences: Quantum computing in a few sentences sentences sentences in a quantum computing quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum quantum\n",
            "ONNX text generation time: 2914.28 ms\n",
            "Complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97269b69",
        "outputId": "b3a9010d-2c59-499d-a17f-873340bc716d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper function to prepare ONNX inputs with padding/truncation\n",
        "def prepare_onnx_inputs_for_step(input_id_token, dynamic_attention_mask, dynamic_past_kv, session_seq_len, num_layers, num_heads, head_dim):\n",
        "    # ONNX input_ids is always (BATCH_SIZE, 1)\n",
        "    onnx_input_ids = input_id_token\n",
        "\n",
        "    # ONNX attention_mask is always (BATCH_SIZE, session_seq_len)\n",
        "    onnx_attention_mask = np.zeros((BATCH_SIZE, session_seq_len), dtype=np.int64)\n",
        "    current_mask_len = dynamic_attention_mask.shape[1]\n",
        "    if current_mask_len > session_seq_len:\n",
        "        onnx_attention_mask[:, :] = dynamic_attention_mask[:, -session_seq_len:]\n",
        "    else:\n",
        "        # Pad with zeros at the beginning\n",
        "        onnx_attention_mask[:, session_seq_len - current_mask_len:] = dynamic_attention_mask\n",
        "\n",
        "    # ONNX past_key_values are always (BATCH_SIZE, NUM_HEADS, session_seq_len, HEAD_DIM)\n",
        "    onnx_past_key_values = []\n",
        "    for j in range(num_layers):\n",
        "        current_k = dynamic_past_kv[2*j]\n",
        "        current_v = dynamic_past_kv[2*j+1]\n",
        "\n",
        "        onnx_k = np.zeros((BATCH_SIZE, num_heads, session_seq_len, head_dim), dtype=np.float32)\n",
        "        onnx_v = np.zeros((BATCH_SIZE, num_heads, session_seq_len, head_dim), dtype=np.float32)\n",
        "\n",
        "        current_kv_len = current_k.shape[2]\n",
        "        if current_kv_len > session_seq_len:\n",
        "            onnx_k[:, :, :, :] = current_k[:, :, -session_seq_len:, :]\n",
        "            onnx_v[:, :, :, :] = current_v[:, :, -session_seq_len:, :]\n",
        "        else:\n",
        "            # Pad with zeros at the beginning\n",
        "            onnx_k[:, :, session_seq_len - current_kv_len:, :] = current_k\n",
        "            onnx_v[:, :, session_seq_len - current_kv_len:, :] = current_v\n",
        "\n",
        "        onnx_past_key_values.extend([onnx_k, onnx_v])\n",
        "\n",
        "    inputs_dict = {\n",
        "        \"input_ids\": onnx_input_ids,\n",
        "        \"attention_mask\": onnx_attention_mask\n",
        "    }\n",
        "    for j in range(num_layers):\n",
        "        inputs_dict[f\"past_key_{2*j}\"] = onnx_past_key_values[2*j]\n",
        "        inputs_dict[f\"past_value_{j}\"] = onnx_past_key_values[2*j + 1]\n",
        "    return inputs_dict\n",
        "\n",
        "\n",
        "print(\"\\nBenchmarking ONNX Runtime static KV cache inference...\")\n",
        "\n",
        "initial_prompt_benchmark = \"The quick brown fox\"\n",
        "encoded_input_benchmark = tokenizer(\n",
        "    initial_prompt_benchmark,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\", # We'll handle this dynamically for ONNX inputs\n",
        "    truncation=True,\n",
        "    max_length=SEQ_LEN # Use SEQ_LEN for max prompt length\n",
        ")\n",
        "\n",
        "# Initialize dynamic states for the benchmarking loop, similar to `generate` method\n",
        "# These states will dynamically grow (current_attention_mask, current_past_kv)\n",
        "# The `input_id_token` for ONNX will always be (1,1)\n",
        "current_input_ids_dynamic = None # Will hold the single input token for each step\n",
        "current_attention_mask_dynamic = np.array([], dtype=np.int64).reshape(BATCH_SIZE, 0)\n",
        "current_past_kv_dynamic = [\n",
        "    np.zeros((BATCH_SIZE, NUM_HEADS, 0, HEAD_DIM), dtype=np.float32)\n",
        "    for _ in range(NUM_LAYERS * 2)\n",
        "]\n",
        "\n",
        "\n",
        "# --- Process prompt tokens first to build initial KV cache ---\n",
        "prompt_ids = encoded_input_benchmark.input_ids\n",
        "prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "for token_idx in range(prompt_len):\n",
        "    input_id_token = prompt_ids[:, token_idx:token_idx+1]\n",
        "\n",
        "    current_attention_mask_dynamic = np.concatenate([current_attention_mask_dynamic, np.array([[1]])], axis=-1)\n",
        "\n",
        "    temp_onnx_inputs = prepare_onnx_inputs_for_step(\n",
        "        input_id_token,\n",
        "        current_attention_mask_dynamic,\n",
        "        current_past_kv_dynamic,\n",
        "        SEQ_LEN, NUM_LAYERS, NUM_HEADS, HEAD_DIM\n",
        "    )\n",
        "\n",
        "    outputs = onnx_inference_model.session.run(None, temp_onnx_inputs)\n",
        "    new_past_kv = outputs[1:]\n",
        "\n",
        "    current_input_ids_dynamic = input_id_token # Keep track of the last token processed\n",
        "    current_past_kv_dynamic = new_past_kv # Update dynamic KV cache\n",
        "\n",
        "\n",
        "# After processing prompt, `current_input_ids_dynamic`, `current_attention_mask_dynamic`, `current_past_kv_dynamic`\n",
        "# hold the state *after* the prompt. `current_input_ids_dynamic` is the last token of the prompt.\n",
        "\n",
        "\n",
        "# --- Warm-up for new token generation ---\n",
        "for _ in range(5):\n",
        "    temp_onnx_inputs = prepare_onnx_inputs_for_step(\n",
        "        current_input_ids_dynamic,\n",
        "        current_attention_mask_dynamic,\n",
        "        current_past_kv_dynamic,\n",
        "        SEQ_LEN, NUM_LAYERS, NUM_HEADS, HEAD_DIM\n",
        "    )\n",
        "\n",
        "    outputs_warmup = onnx_inference_model.session.run(None, temp_onnx_inputs)\n",
        "    next_token_logits_warmup = outputs_warmup[0][:, -1, :]\n",
        "    next_token_warmup = np.argmax(next_token_logits_warmup, axis=-1)\n",
        "\n",
        "    current_input_ids_dynamic = next_token_warmup[:, np.newaxis]\n",
        "    current_attention_mask_dynamic = np.concatenate([current_attention_mask_dynamic, np.array([[1]])], axis=-1)\n",
        "    current_past_kv_dynamic = outputs_warmup[1:]\n",
        "\n",
        "\n",
        "# --- Reset for actual timing (re-initialize dynamic states and re-process prompt) ---\n",
        "current_input_ids_dynamic = None\n",
        "current_attention_mask_dynamic = np.array([], dtype=np.int64).reshape(BATCH_SIZE, 0)\n",
        "current_past_kv_dynamic = [\n",
        "    np.zeros((BATCH_SIZE, NUM_HEADS, 0, HEAD_DIM), dtype=np.float32)\n",
        "    for _ in range(NUM_LAYERS * 2)\n",
        "]\n",
        "\n",
        "for token_idx in range(prompt_len):\n",
        "    input_id_token = prompt_ids[:, token_idx:token_idx+1]\n",
        "\n",
        "    current_attention_mask_dynamic = np.concatenate([current_attention_mask_dynamic, np.array([[1]])], axis=-1)\n",
        "\n",
        "    temp_onnx_inputs = prepare_onnx_inputs_for_step(\n",
        "        input_id_token,\n",
        "        current_attention_mask_dynamic,\n",
        "        current_past_kv_dynamic,\n",
        "        SEQ_LEN, NUM_LAYERS, NUM_HEADS, HEAD_DIM\n",
        "    )\n",
        "\n",
        "    outputs_reset = onnx_inference_model.session.run(None, temp_onnx_inputs)\n",
        "    new_past_kv = outputs_reset[1:]\n",
        "\n",
        "    current_input_ids_dynamic = input_id_token\n",
        "    current_past_kv_dynamic = new_past_kv\n",
        "\n",
        "\n",
        "# --- Actual timing loop for single token generation ---\n",
        "onnx_kv_times = []\n",
        "for _ in range(20):\n",
        "    start = time.time()\n",
        "\n",
        "    temp_onnx_inputs = prepare_onnx_inputs_for_step(\n",
        "        current_input_ids_dynamic,\n",
        "        current_attention_mask_dynamic,\n",
        "        current_past_kv_dynamic,\n",
        "        SEQ_LEN, NUM_LAYERS, NUM_HEADS, HEAD_DIM\n",
        "    )\n",
        "\n",
        "    outputs_timed = onnx_inference_model.session.run(None, temp_onnx_inputs)\n",
        "    end = time.time()\n",
        "    onnx_kv_times.append(end - start)\n",
        "\n",
        "    next_token_logits_timed = outputs_timed[0][:, -1, :]\n",
        "    next_token_timed = np.argmax(next_token_logits_timed, axis=-1)\n",
        "\n",
        "    current_input_ids_dynamic = next_token_timed[:, np.newaxis]\n",
        "    current_attention_mask_dynamic = np.concatenate([current_attention_mask_dynamic, np.array([[1]])], axis=-1)\n",
        "    current_past_kv_dynamic = outputs_timed[1:]\n",
        "\n",
        "onnx_kv_mean_time = np.mean(onnx_kv_times) * 1000\n",
        "onnx_kv_std_time = np.std(onnx_kv_times) * 1000\n",
        "\n",
        "print(f\"ONNX static KV cache inference time (per token): {onnx_kv_mean_time:.2f} ms ± {onnx_kv_std_time:.2f} ms\")\n",
        "print(\"Complete\\n\")\n",
        "\n",
        "print(\"\\n--- Performance Comparison ---\")\n",
        "print(f\"PyTorch (full sequence): {pytorch_mean_time:.2f} ms ± {pytorch_std_time:.2f} ms\")\n",
        "print(f\"ONNX (simple inference): {mean_time:.2f} ms ± {std_time:.2f} ms\")\n",
        "print(f\"ONNX (static KV cache, per token): {onnx_kv_mean_time:.2f} ms ± {onnx_kv_std_time:.2f} ms\")\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Benchmarking ONNX Runtime static KV cache inference...\n",
            "ONNX static KV cache inference time (per token): 43.72 ms ± 1.36 ms\n",
            "Complete\n",
            "\n",
            "\n",
            "--- Performance Comparison ---\n",
            "PyTorch (full sequence): 354.79 ms ± 115.15 ms\n",
            "ONNX (simple inference): 77.01 ms ± 2.67 ms\n",
            "ONNX (static KV cache, per token): 43.72 ms ± 1.36 ms\n"
          ]
        }
      ]
    }
  ]
}