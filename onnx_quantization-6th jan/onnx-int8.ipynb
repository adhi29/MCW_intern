{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4336109,"sourceType":"datasetVersion","datasetId":2553087},{"sourceId":14410799,"sourceType":"datasetVersion","datasetId":9203852},{"sourceId":710788,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":540017,"modelId":553206}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \\\n    onnx \\\n    onnxruntime \\\n    onnxruntime-tools \\\n    tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T10:22:00.075561Z","iopub.execute_input":"2026-01-06T10:22:00.075762Z","iopub.status.idle":"2026-01-06T10:22:06.765661Z","shell.execute_reply.started":"2026-01-06T10:22:00.075741Z","shell.execute_reply":"2026-01-06T10:22:06.764776Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\nimport os\n\nFP32_MODEL_SRC = \"/kaggle/input/inception-v3/onnx/default/1/inception_v3.onnx\"\nFP32_MODEL_LOCAL = \"/kaggle/working/inception_v3.onnx\"\n\nif not os.path.exists(FP32_MODEL_LOCAL):\n    shutil.copy(FP32_MODEL_SRC, FP32_MODEL_LOCAL)\n\nprint(\"Copied model to writable path:\", FP32_MODEL_LOCAL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T11:01:21.100104Z","iopub.execute_input":"2026-01-06T11:01:21.100462Z","iopub.status.idle":"2026-01-06T11:01:21.168249Z","shell.execute_reply.started":"2026-01-06T11:01:21.100418Z","shell.execute_reply":"2026-01-06T11:01:21.167671Z"}},"outputs":[{"name":"stdout","text":"Copied model to writable path: /kaggle/working/inception_v3.onnx\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import onnxruntime as ort\nfrom onnxruntime.quantization import quantize_static, QuantType, CalibrationDataReader\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nimport time\nimport os\nimport shutil\nfrom tqdm import tqdm\n\n# =====================================================\n# PATHS (KAGGLE - VERIFIED)\n# =====================================================\nIMAGENET_PATH = \"/kaggle/input/imagenet1kvalid\"\n\n# Original model in read-only input directory\nORIGINAL_FP32_PATH = \"/kaggle/input/inception-v3/onnx/default/1/inception_v3.onnx\"\n\n# Copy to writable directory\nFP32_MODEL_PATH = \"/kaggle/working/inception_v3_fp32.onnx\"\nINT8_MODEL_PATH = \"/kaggle/working/inception_v3_int8.onnx\"\n\nprint(\"Original FP32 model exists:\", os.path.isfile(ORIGINAL_FP32_PATH))\nprint(\"ImageNet exists:\", os.path.isdir(IMAGENET_PATH))\n\nprint(\"\\nCopying FP32 model to writable directory...\")\nshutil.copy(ORIGINAL_FP32_PATH, FP32_MODEL_PATH)\nprint(\"✓ Model copied to:\", FP32_MODEL_PATH)\n\n# CALIBRATION DATA READER\n\nclass InceptionCalibrationDataReader(CalibrationDataReader):\n    def __init__(self, calibration_dataset, batch_size=16):\n        self.loader = DataLoader(\n            calibration_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=2\n        )\n        self.iterator = iter(self.loader)\n        self.input_name = \"input\"\n\n    def get_next(self):\n        try:\n            images, _ = next(self.iterator)\n            return {self.input_name: images.numpy()}\n        except StopIteration:\n            return None\n\n\n# PREPROCESSING\n\nprint(\"\\n[1] Preparing preprocessing...\")\n\npreprocess = transforms.Compose([\n    transforms.Resize(342),\n    transforms.CenterCrop(299),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\nprint(\"✓ Preprocessing ready\")\n\n# DATASET\n\nprint(\"\\n[2] Loading ImageNet validation dataset...\")\n\nval_dataset = datasets.ImageFolder(\n    root=IMAGENET_PATH,\n    transform=preprocess\n)\n\n# small numbers first (good practice)\ncalibration_dataset = Subset(val_dataset, range(1000))\ninference_dataset = Subset(val_dataset, range(5000))\n\nprint(\"✓ Dataset loaded\")\nprint(\"  Calibration images:\", len(calibration_dataset))\nprint(\"  Inference images:\", len(inference_dataset))\n\n# STATIC QUANTIZATION\n\nprint(\"\\n[3] Running static INT8 quantization...\")\n\ncalibration_reader = InceptionCalibrationDataReader(calibration_dataset)\n\nquantize_static(\n    model_input=FP32_MODEL_PATH,\n    model_output=INT8_MODEL_PATH,\n    calibration_data_reader=calibration_reader,\n    quant_format=QuantType.QInt8,\n    per_channel=False,\n    weight_type=QuantType.QInt8,\n    activation_type=QuantType.QInt8,\n)\n\nprint(\"✓ INT8 model saved to:\", INT8_MODEL_PATH)\n\n\n# INFERENCE FUNCTION (WITH PROGRESS BAR)\n\ndef run_inference(model_path, loader, tag):\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    session = ort.InferenceSession(model_path, providers=providers)\n    input_name = session.get_inputs()[0].name\n    \n    print(f\"  Active provider: {session.get_providers()[0]}\")\n\n    correct1 = correct5 = total = 0\n    start = time.time()\n\n    for images, labels in tqdm(loader, desc=tag):\n        outputs = session.run(None, {input_name: images.numpy()})\n        logits = outputs[0]\n\n        pred1 = np.argmax(logits, axis=1)\n        pred5 = np.argsort(logits, axis=1)[:, -5:]\n\n        correct1 += (pred1 == labels.numpy()).sum()\n        correct5 += sum(\n            label in pred5[i]\n            for i, label in enumerate(labels.numpy())\n        )\n        total += labels.size(0)\n\n    elapsed = time.time() - start\n\n    print(\n        f\"\\n{tag} | \"\n        f\"Top-1: {100 * correct1 / total:.2f}% | \"\n        f\"Top-5: {100 * correct5 / total:.2f}% | \"\n        f\"Time: {elapsed:.2f}s\"\n    )\n\n    return elapsed\n\n\nprint(\"\\n[4] Running evaluation...\")\n\ninference_loader = DataLoader(\n    inference_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=2\n)\n\nprint(\"\\nFP32 inference...\")\nfp32_time = run_inference(FP32_MODEL_PATH, inference_loader, \"FP32\")\n\nprint(\"\\nINT8 inference...\")\nint8_time = run_inference(INT8_MODEL_PATH, inference_loader, \"INT8\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Speed-up: {fp32_time / int8_time:.2f}x\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T11:12:33.471474Z","iopub.execute_input":"2026-01-06T11:12:33.472224Z","iopub.status.idle":"2026-01-06T11:25:13.006548Z","shell.execute_reply.started":"2026-01-06T11:12:33.472191Z","shell.execute_reply":"2026-01-06T11:25:13.005710Z"}},"outputs":[{"name":"stdout","text":"Original FP32 model exists: True\nImageNet exists: True\n\nCopying FP32 model to writable directory...\n✓ Model copied to: /kaggle/working/inception_v3_fp32.onnx\n\n[1] Preparing preprocessing...\n✓ Preprocessing ready\n\n[2] Loading ImageNet validation dataset...\n✓ Dataset loaded\n  Calibration images: 1000\n  Inference images: 5000\n\n[3] Running static INT8 quantization...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \nWARNING:root:Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\nWARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:123: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✓ INT8 model saved to: /kaggle/working/inception_v3_int8.onnx\n\n[4] Running evaluation...\n\nFP32 inference...\n  Active provider: CPUExecutionProvider\n","output_type":"stream"},{"name":"stderr","text":"FP32: 100%|██████████| 313/313 [05:10<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFP32 | Top-1: 82.16% | Top-5: 96.08% | Time: 310.99s\n\nINT8 inference...\n  Active provider: CPUExecutionProvider\n","output_type":"stream"},{"name":"stderr","text":"INT8: 100%|██████████| 313/313 [05:19<00:00,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"\nINT8 | Top-1: 81.14% | Top-5: 95.56% | Time: 319.41s\n\n==================================================\nSpeed-up: 0.97x\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import onnxruntime as ort\nfrom onnxruntime.quantization import quantize_static, QuantType, CalibrationDataReader\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nimport time\nimport os\nimport shutil\nfrom tqdm import tqdm\n\n# =====================================================\n# PATHS (KAGGLE - VERIFIED)\n# =====================================================\nIMAGENET_PATH = \"/kaggle/input/imagenet1kvalid\"\n\nORIGINAL_FP32_PATH = \"/kaggle/input/inception-v3/onnx/default/1/inception_v3.onnx\"\n\nFP32_MODEL_PATH = \"/kaggle/working/inception_v3_fp32.onnx\"\nINT8_ASYM_MODEL_PATH = \"/kaggle/working/inception_v3_int8_asymmetric.onnx\"\n\nprint(\"Original FP32 model exists:\", os.path.isfile(ORIGINAL_FP32_PATH))\nprint(\"ImageNet exists:\", os.path.isdir(IMAGENET_PATH))\n\nprint(\"\\nCopying FP32 model to writable directory...\")\nshutil.copy(ORIGINAL_FP32_PATH, FP32_MODEL_PATH)\nprint(\"✓ Model copied to:\", FP32_MODEL_PATH)\n\n# =====================================================\n# CALIBRATION DATA READER\n# =====================================================\nclass InceptionCalibrationDataReader(CalibrationDataReader):\n    def __init__(self, calibration_dataset, batch_size=16):\n        self.loader = DataLoader(\n            calibration_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=2\n        )\n        self.iterator = iter(self.loader)\n        self.input_name = \"input\"\n\n    def get_next(self):\n        try:\n            images, _ = next(self.iterator)\n            return {self.input_name: images.numpy()}\n        except StopIteration:\n            return None\n\n# =====================================================\n# PREPROCESSING\n# =====================================================\nprint(\"\\n[1] Preparing preprocessing...\")\n\npreprocess = transforms.Compose([\n    transforms.Resize(342),\n    transforms.CenterCrop(299),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    ),\n])\n\nprint(\"✓ Preprocessing ready\")\n\n# =====================================================\n# DATASET\n# =====================================================\nprint(\"\\n[2] Loading ImageNet validation dataset...\")\n\nval_dataset = datasets.ImageFolder(\n    root=IMAGENET_PATH,\n    transform=preprocess\n)\n\ncalibration_dataset = Subset(val_dataset, range(1000))\ninference_dataset = Subset(val_dataset, range(5000))\n\nprint(\"✓ Dataset loaded\")\nprint(\"  Calibration images:\", len(calibration_dataset))\nprint(\"  Inference images:\", len(inference_dataset))\n\n# =====================================================\n# ASYMMETRIC STATIC QUANTIZATION\n# =====================================================\nprint(\"\\n[3] Running STATIC ASYMMETRIC INT8 quantization...\")\n\ncalibration_reader = InceptionCalibrationDataReader(calibration_dataset)\n\nquantize_static(\n    model_input=FP32_MODEL_PATH,\n    model_output=INT8_ASYM_MODEL_PATH,\n    calibration_data_reader=calibration_reader,\n    weight_type=QuantType.QInt8,       # symmetric weights\n    activation_type=QuantType.QUInt8,  # asymmetric activations\n    per_channel=False,\n)\n\nprint(\"✓ Asymmetric INT8 model saved to:\", INT8_ASYM_MODEL_PATH)\n\n# =====================================================\n# INFERENCE FUNCTION\n# =====================================================\ndef run_inference(model_path, loader, tag):\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    session = ort.InferenceSession(model_path, providers=providers)\n\n    input_name = session.get_inputs()[0].name\n    print(f\"  Active provider: {session.get_providers()[0]}\")\n\n    correct1 = correct5 = total = 0\n    start = time.time()\n\n    for images, labels in tqdm(loader, desc=tag):\n        outputs = session.run(None, {input_name: images.numpy()})\n        logits = outputs[0]\n\n        pred1 = np.argmax(logits, axis=1)\n        pred5 = np.argsort(logits, axis=1)[:, -5:]\n\n        correct1 += (pred1 == labels.numpy()).sum()\n        correct5 += sum(\n            label in pred5[i]\n            for i, label in enumerate(labels.numpy())\n        )\n        total += labels.size(0)\n\n    elapsed = time.time() - start\n\n    print(\n        f\"\\n{tag} | \"\n        f\"Top-1: {100 * correct1 / total:.2f}% | \"\n        f\"Top-5: {100 * correct5 / total:.2f}% | \"\n        f\"Time: {elapsed:.2f}s\"\n    )\n\n    return elapsed\n\n# =====================================================\n# EVALUATION\n# =====================================================\nprint(\"\\n[4] Running evaluation...\")\n\ninference_loader = DataLoader(\n    inference_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=2\n)\n\nprint(\"\\nFP32 inference...\")\nfp32_time = run_inference(FP32_MODEL_PATH, inference_loader, \"FP32\")\n\nprint(\"\\nINT8 ASYMMETRIC inference...\")\nint8_time = run_inference(INT8_ASYM_MODEL_PATH, inference_loader, \"INT8-ASYM\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Speed-up: {fp32_time / int8_time:.2f}x\")\nprint(\"=\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T11:48:56.305120Z","iopub.execute_input":"2026-01-06T11:48:56.305570Z","iopub.status.idle":"2026-01-06T12:01:37.203516Z","shell.execute_reply.started":"2026-01-06T11:48:56.305534Z","shell.execute_reply":"2026-01-06T12:01:37.202691Z"}},"outputs":[{"name":"stdout","text":"Original FP32 model exists: True\nImageNet exists: True\n\nCopying FP32 model to writable directory...\n✓ Model copied to: /kaggle/working/inception_v3_fp32.onnx\n\n[1] Preparing preprocessing...\n✓ Preprocessing ready\n\n[2] Loading ImageNet validation dataset...\n✓ Dataset loaded\n  Calibration images: 1000\n  Inference images: 5000\n\n[3] Running STATIC ASYMMETRIC INT8 quantization...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \nWARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","output_type":"stream"},{"name":"stdout","text":"✓ Asymmetric INT8 model saved to: /kaggle/working/inception_v3_int8_asymmetric.onnx\n\n[4] Running evaluation...\n\nFP32 inference...\n  Active provider: CPUExecutionProvider\n","output_type":"stream"},{"name":"stderr","text":"FP32: 100%|██████████| 313/313 [05:18<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nFP32 | Top-1: 82.16% | Top-5: 96.08% | Time: 318.36s\n\nINT8 ASYMMETRIC inference...\n  Active provider: CPUExecutionProvider\n","output_type":"stream"},{"name":"stderr","text":"INT8-ASYM: 100%|██████████| 313/313 [04:36<00:00,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"\nINT8-ASYM | Top-1: 81.30% | Top-5: 95.64% | Time: 276.19s\n\n==================================================\nSpeed-up: 1.15x\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12}]}