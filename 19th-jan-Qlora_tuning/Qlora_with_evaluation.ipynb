{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyj00lgXeyZ5",
        "outputId": "71846f6a-1f9d-4997-ca94-cea7f3a90163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoTFIqFH5H-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers datasets peft bitsandbytes accelerate tqdm"
      ],
      "metadata": {
        "id": "GjM2ufX9dZ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes>=0.41.0"
      ],
      "metadata": {
        "id": "MDH579UdeAOB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import math\n",
        "\n",
        "model_name = \"roneneldan/TinyStories-1M\"\n",
        "block_size = 128\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "lr = 3e-4\n",
        "\n",
        "adapter_path = \"tinystories-1m-qlora-adapters\"\n",
        "merged_path = \"tinystories-1m-qlora-merged\"\n",
        "\n",
        "# grab shakespeare data\n",
        "dataset = load_dataset(\"text\", data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    all_masks = sum(examples[\"attention_mask\"], [])\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"attention_mask\": [all_masks[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"labels\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    }\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(lm_dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# check perplexity\n",
        "def calc_perplexity(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "            mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "            labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "            out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            total_loss += out.loss.item() * ids.size(0)\n",
        "            count += ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    return math.exp(avg_loss), avg_loss\n",
        "\n",
        "# baseline check\n",
        "print(\"\\ngetting baseline scores...\")\n",
        "base_model_check = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "base_ppl, base_loss = calc_perplexity(base_model_check, test_loader)\n",
        "\n",
        "print(f\"baseline - loss: {base_loss:.4f}, perplexity: {base_ppl:.2f}\")\n",
        "\n",
        "del base_model_check\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# setup 4bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# train\n",
        "print(f\"\\ntraining for {epochs} epoch(s)...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nepoch {epoch+1}\")\n",
        "    running_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        loss = out.loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"avg loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(\"\\nmerging model...\")\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "model = PeftModel.from_pretrained(base, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "model.save_pretrained(merged_path)\n",
        "tokenizer.save_pretrained(merged_path)\n",
        "\n",
        "# eval after training\n",
        "print(\"\\nchecking finetuned model...\")\n",
        "ft_ppl, ft_loss = calc_perplexity(model, test_loader)\n",
        "\n",
        "print(f\"finetuned - loss: {ft_loss:.4f}, perplexity: {ft_ppl:.2f}\")\n",
        "\n",
        "# compare\n",
        "loss_change = ((base_loss - ft_loss) / base_loss) * 100\n",
        "ppl_change = ((base_ppl - ft_ppl) / base_ppl) * 100\n",
        "\n",
        "print(f\"\\n--- results ---\")\n",
        "print(f\"loss: {base_loss:.4f} -> {ft_loss:.4f} ({loss_change:+.1f}%)\")\n",
        "print(f\"perplexity: {base_ppl:.2f} -> {ft_ppl:.2f} ({ppl_change:+.1f}%)\")\n",
        "\n",
        "# test generation\n",
        "print(\"\\n--- sample generations ---\")\n",
        "base_gen = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "prompts = [\"Once upon a time\", \"To be or not to be\"]\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nprompt: {p}\")\n",
        "\n",
        "    inp = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        base_out = base_gen.generate(**inp, max_new_tokens=50, temperature=0.9, do_sample=True)\n",
        "        ft_out = model.generate(**inp, max_new_tokens=50, temperature=0.9, do_sample=True)\n",
        "\n",
        "    print(f\"before: {tokenizer.decode(base_out[0], skip_special_tokens=True)}\")\n",
        "    print(f\"after: {tokenizer.decode(ft_out[0], skip_special_tokens=True)}\")\n",
        "\n",
        "del base_gen\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# download\n",
        "shutil.make_archive(merged_path, \"zip\", merged_path)\n",
        "files.download(f\"{merged_path}.zip\")\n",
        "print(\"\\ndone!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "49bb6d1e69d54be69e7ae01a69444be9",
            "d508449c689d4e028bd13b8ed646502e",
            "6c8076d365f243599e85be9cfbc1f417",
            "87f12a90af5d4709a3aa5d6f7e58e1ec",
            "a80fd818284344debd82b634c3590ebd",
            "ed5c2c8a0e42413daed29ae6754c14da",
            "097bd523d3b44edd80271d46b43e1ce4",
            "88e7df61f40f454f9caad7979f123e81",
            "ba42c8f03dd341049d7bacd876111976",
            "3fccbc2d25f34388925b00ecc3e0780a",
            "26ac6cb25e3c4965bc74cce3855df52b",
            "2569e4997e9f4a509d406041661da3e1",
            "169fdb8e5f4d460e8571e626fb7ec1ff",
            "7495f59477f247ee98153395f67ab2d2",
            "26fe7ed4d38146749fff1a2ada8a15f2",
            "5d3cc0c33c2140899957315a410ce640",
            "89e89d5a233f4eda850af1bdf31be9eb",
            "bc06713fdb5945f38436e4419bcd7d9d",
            "4f80f738c0f3495b82dc04543266b1eb",
            "ad21c539de7547ae8d0fab77abcfd2a9",
            "591034698b54478986a3ef26dc86f49b",
            "dc6285da4ac3486995eb42012da28c18",
            "760c5a4bf670468f83a6f31ae627bbb7",
            "b1edb2513e98449b9ddaa97e6c506464",
            "0e890b5538f74d53b349253e5799f9ea",
            "f4a9f235db8d4a0b9af27bb1adfaf716",
            "db81bc15f8ef4a35a1b0ee81e9fd9621",
            "a7953ef61d76406ba2015cbd6571a9d0",
            "3c1086ea2b584430923b38ea287b6984",
            "e4e60267661941feb87d9b29e9cdb633",
            "2b49342f226d4968855630c5038fe82d",
            "af3a9eb5785b4154bfd7179b234770dd",
            "c51949e44347433b8997eb19400ec123",
            "f007c3aec109427e827e35a85816a4d3",
            "8c23e06f35334db9993ab2c9d5470036",
            "0b5dd7863ea34352ba440d5a9523321e",
            "5203bd870e654bc5b08f9e59b1f05c85",
            "0d5375527c7e452db03935c08902fdab",
            "e6cdd347762841a78597c565246dc722",
            "374b712eb8c349058d1c560372514d62",
            "e6db1a0b82014f848e930fb9d15c5346",
            "9bfc35ba67564bebbc371179dcbaf440",
            "edaa73164a444cbe9539e4c8e77179d3",
            "a769c0bc861c45ad8a06f568d6c8d7fc"
          ]
        },
        "id": "D31atYTE5Jxq",
        "outputId": "e10d80f7-1d69-4bd2-dec7-84efbb84b829"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49bb6d1e69d54be69e7ae01a69444be9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2569e4997e9f4a509d406041661da3e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "760c5a4bf670468f83a6f31ae627bbb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f007c3aec109427e827e35a85816a4d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "getting baseline scores...\n",
            "baseline - loss: 9.3344, perplexity: 11320.67\n",
            "trainable params: 16,384 || all params: 3,762,368 || trainable%: 0.4355\n",
            "\n",
            "training for 20 epoch(s)...\n",
            "\n",
            "epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.8677\n",
            "\n",
            "epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.5784\n",
            "\n",
            "epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.4836\n",
            "\n",
            "epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:25<00:00, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.3916\n",
            "\n",
            "epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.3061\n",
            "\n",
            "epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.2646\n",
            "\n",
            "epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.2366\n",
            "\n",
            "epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.2144\n",
            "\n",
            "epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1961\n",
            "\n",
            "epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1802\n",
            "\n",
            "epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1649\n",
            "\n",
            "epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1549\n",
            "\n",
            "epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1437\n",
            "\n",
            "epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1348\n",
            "\n",
            "epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1233\n",
            "\n",
            "epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1185\n",
            "\n",
            "epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1116\n",
            "\n",
            "epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.1028\n",
            "\n",
            "epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.0977\n",
            "\n",
            "epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg loss: 7.0909\n",
            "\n",
            "merging model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "checking finetuned model...\n",
            "finetuned - loss: 7.4547, perplexity: 1728.04\n",
            "\n",
            "--- results ---\n",
            "loss: 9.3344 -> 7.4547 (+20.1%)\n",
            "perplexity: 11320.67 -> 1728.04 (+84.7%)\n",
            "\n",
            "--- sample generations ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "prompt: Once upon a time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: Once upon a time, in a big room, there were two sisters. The sisters in the kitchen were very happy, they were always playing with their toys.\n",
            "\n",
            "after: Once upon a time, was and and leave his dad,Not mind, let his promise his chest, he can hurt and not and prevent himself that stay your mind.You must be?But, because do be stupid:But my son, no dollar: let your\n",
            "\n",
            "prompt: To be or not to be\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: To be or not to be. I am sorry and she won't be mad at me. I am sorry, Max. I will find Max and True. Please, Max. I will stop you and hug them.\"\n",
            "\n",
            "Max and Max are happy. They hug Max.\n",
            "after: To be or not to be very important than he he be knew well before,\"?Sure, he always weigh no - no his boat is not his cub, if you will keep his mind and promise to his dad, that and that?\"And he do no people who because of\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_888dda0e-b800-4ea6-ace2-4505e3245ecb\", \"tinystories-1m-qlora-merged.zip\", 7987362)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987,
          "referenced_widgets": [
            "744af888031548a798ba913f92323e56",
            "0f66cab56d7c430cb99919068c5918dc",
            "c13ba6f02d12463083f37a6e14edce39",
            "7cf0219fe8f340ef8eb8d881d0ae611a",
            "e77b9b0e66eb470f8b7bdfc88b50e92f",
            "6541a79e5be54cdfbbbde17f7ff3d839",
            "ef4446155d5e451b95c494d0e63a9cca",
            "6580a53081de4f6aac38ad2a0e33bd3b",
            "54b6a62db80c4e0db418436369019299",
            "44522c9230b546eebabca4bc76fae22e",
            "bf38dfe2f63a44079d76281154606557",
            "e29b67c5b90e4ef081f8917edb7891b5",
            "690a549bb42f45f4b91f8a4275548cb7",
            "23c1a4bbbb1a43f59cc90eb2b0f6f92f",
            "36f7d4e4f277485383aa496320485f95",
            "edd5fda38cb043a480619ec4155c9368",
            "48aa2c920f544031a0c70d57a3b0caab",
            "c78d2c4ce83e4b0986eaee372cf97857",
            "2d2c396e8d5f4d74ad89a6f50f47a6bd",
            "907f0a4de04a4b46943c4e047210706c",
            "92c3aa4bb21a4727a90761edcb968d6a",
            "6bb0ed25f0e0405a8c5189669a4c2d11",
            "19402ccdfcf840fb9322258e8e5de2a1",
            "a5a374d17d6e49788644f3c471ff40ff",
            "317a7f0ee80743578d2ea09d7ef2e1e6",
            "958d531e835545a5b2b2f98548418131",
            "8415db61e6874475abd2b2da8b60dc05",
            "f0e73ec97aaa45d587d313322d4793f2",
            "8687824ab4944fa0ac7aa919da63793a",
            "9e7ebb51a082450c81ad130014aaf3d1",
            "d7876c71e1464e4d86f8b6be5ce6dfd0",
            "d8d8da643c2f431387e5bdecfed4a99a",
            "b93204d3270642168726ebec81f7f3db",
            "c795cd8cb0044ac788e0e5f5013892b1",
            "bbd12bff03c04372a9666757fba51a92",
            "01c59a6cf41442f2b236555e4326adc2",
            "f48968cb98f54f25adeaf59f6431bfc0",
            "0bf95bdfe1124f8984e17bc6cea90e47",
            "a7348b8bd7ec4047857edf4cacc03f44",
            "92e25bd5438344a9b0c971bd703a5b11",
            "17134f20ab39441ebdcd26c29d76434a",
            "1cba174477b84caa8e43f6463b96c4f8",
            "e0064ae410004ef4b7c57cb2c7f0af65",
            "19d097c08a974f2b92f6b796ce2b49dc"
          ]
        },
        "id": "OLHiMZQ2ZZQ6",
        "outputId": "3e515383-d0a1-4e3b-dbcb-1bb1bb363a73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "744af888031548a798ba913f92323e56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e29b67c5b90e4ef081f8917edb7891b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/36000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19402ccdfcf840fb9322258e8e5de2a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c795cd8cb0044ac788e0e5f5013892b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 16,384 || all params: 3,762,368 || trainable%: 0.4355\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 14.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:17<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:18<00:00, 14.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:21<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:18<00:00, 14.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 260/260 [00:16<00:00, 16.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging adapters with base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_afc2ee74-deaf-4114-a5cd-cee7ea2a9876\", \"tinystories-1m-qlora-merged.zip\", 7987328)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "Sun rises in the east side,Sure, please be be be happy - when he knew that?Relupid and promise you will stay him well that that were no,\"Poor worry because are terrible.A be foolish but in and stop.I can lose his fate to?Sure please no worry but surrender for this punishment \u2013No good his coin with the prison,Of worry, no, please to be happy \u2013He be\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# config stuff\n",
        "model_name = \"roneneldan/TinyStories-1M\"\n",
        "block_size = 128\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "lr = 3e-4\n",
        "\n",
        "adapter_path = \"tinystories-1m-qlora-adapters\"\n",
        "merged_path = \"tinystories-1m-qlora-merged\"\n",
        "\n",
        "# load tiny shakespeare\n",
        "dataset = load_dataset(\"text\", data_files=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# tokenize the data\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# chunk into blocks\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    all_masks = sum(examples[\"attention_mask\"], [])\n",
        "\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "\n",
        "    result = {\n",
        "        \"input_ids\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"attention_mask\": [all_masks[i:i+block_size] for i in range(0, total_len, block_size)],\n",
        "        \"labels\": [all_ids[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    }\n",
        "    return result\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "\n",
        "# dataloader\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(lm_dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# load model in 4bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "# add lora adapters\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# training\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"labels\"].to(\"cuda\")\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# save the lora adapters\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(\"Merging adapters with base model...\")\n",
        "\n",
        "# reload base in fp16 and merge\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# save merged model\n",
        "model.save_pretrained(merged_path)\n",
        "tokenizer.save_pretrained(merged_path)\n",
        "\n",
        "# zip and download\n",
        "shutil.make_archive(merged_path, \"zip\", merged_path)\n",
        "files.download(f\"{merged_path}.zip\")\n",
        "\n",
        "# quick test\n",
        "model.eval()\n",
        "prompt = \"Sun rises in the east\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=80,\n",
        "        temperature=0.9,\n",
        "        do_sample=True,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oOZUOVeo5F6w"
      }
    }
  ]
}